# Elastic-Net-Regression
Elastic Net Regression-model_1

## ๐ **Elastic Net Regression (ุงูุงูุญุฏุงุฑ ุงูุดุจูู ุงููุฑู)**

### ๐ ูุง ูู Elastic Net Regressionุ

ูู ูููุฐุฌ ุงูุญุฏุงุฑ ุฎุทู ูููู ุจุฏูุฌ ููุฒุงุช ูู ูู:
โ **Lasso Regression (L1 Regularization):** ุงูุฐู ูููู ุงูุฃูุฒุงู ููุฌุนู ุจุนุถูุง = ุตูุฑ (ุงุฎุชูุงุฑ ุงููููุฒุงุช).
โ **Ridge Regression (L2 Regularization):** ุงูุฐู ูููู ุงูุฃูุฒุงู ุงููุจูุฑุฉ ุจุฏูู ุชุตููุฑูุง ุจุงููุงูู.

โจ ุจุงุณุชุฎุฏุงู **Elastic Net**ุ ูุญุตู ุนูู ุงูุชูุงุฒู ุงููุซุงูู ุจููููุง ููุชุญูู ูู ุชุนููุฏ ุงููููุฐุฌ ูุชูููู **Overfitting**.

---

### ๐งฎ **ุงููุนุงุฏูุฉ ุงูุฃุณุงุณูุฉ**

#### 1๏ธโฃ ุฏุงูุฉ ุงูุงูุญุฏุงุฑ:

$$
\hat{y} = b + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

---

#### 2๏ธโฃ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูุน Elastic Net:

$$
\text{Loss} = \underbrace{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}_{\text{MSE}} + \lambda \cdot \left[\alpha \sum |w_j| + (1-\alpha)\sum w_j^2\right]
$$

โ **MSE**: ูุชูุณุท ุงูุฎุทุฃ ุงูุชุฑุจูุนู.
โ $\lambda$: ูุนุงูู ุงูุนููุจุฉ (Regularization strength).
โ $\alpha$: ููุงุฒูุฉ ุจูู L1 ู L2:

* ุฅุฐุง $\alpha=1$ โ ูุตุจุญ ุงููููุฐุฌ Lasso.
* ุฅุฐุง $\alpha=0$ โ ูุตุจุญ ุงููููุฐุฌ Ridge.
* ุฅุฐุง $0<\alpha<1$ โ ูุญุตู ุนูู Elastic Net.

---

### โ๏ธ **ุขููุฉ ุนูู Elastic Net**

1. ูุชู ุญุณุงุจ ุงูุฎุทุฃ ุจูู ุงูููู ุงูุญููููุฉ $(y)$ ูุงูุชูุจุคุงุช $(\hat{y})$.
2. ูุชู ุฅุถุงูุฉ ุนููุจุฉ ุนูู ุงูุฃูุฒุงู:

   * **L1** ุชุฌุจุฑ ุจุนุถ ุงูุฃูุฒุงู ุนูู ุงูุตูุฑ.
   * **L2** ุชููู ูู ุงูุฃูุฒุงู ุงููุจูุฑุฉ.
3. ูุชู ุชุญุฏูุซ ุงููุนุงููุงุช $w, b$ ุจุงุณุชุฎุฏุงู **Gradient Descent**.

---

### ๐๏ธ **ููุฏ Elastic Net Regression ูู ุงูุตูุฑ (Python)**

```python
import numpy as np

# ุจูุงูุงุช ุงูุชุฏุฑูุจ
x = [1, 2, 3]
y = [2, 3, 4]

# ุงููุนุงููุงุช
w = 0.5  # ูุฒู ุงุจุชุฏุงุฆู ุตุบูุฑ
b = 0    # ุซุงุจุช ุงุจุชุฏุงุฆู
alpha = 0.1        # ูุนุฏู ุงูุชุนูู
lmbda = 0.6        # ููุฉ ุงูุนููุจุฉ
elastic_alpha = 0.5  # 0=L2, 1=L1

# ุงูุชุฏุฑูุจ
for epoch in range(100):
    total_error_w = 0
    total_error_b = 0
    mse = 0

    for i in range(len(x)):
        y_pred = b + w * x[i]
        error = y_pred - y[i]
        total_error_w += error * x[i]
        total_error_b += error
        mse += error ** 2

    mse /= (2 * len(x))  # ูุชูุณุท ุงูุฎุทุฃ ุงูุชุฑุจูุนู
    penalty = lmbda * (elastic_alpha * abs(w) + (1 - elastic_alpha) * (w ** 2))
    total_loss = mse + penalty

    # ุงูุชุฏุฑุฌุงุช ูุน Elastic Net
    grad_w = (1 / len(x)) * total_error_w + lmbda * (
        elastic_alpha * np.sign(w) + 2 * (1 - elastic_alpha) * w)
    grad_b = (1 / len(x)) * total_error_b

    # ุชุญุฏูุซ ุงููุนุงููุงุช
    w -= alpha * grad_w
    b -= alpha * grad_b

    if epoch % 10 == 0:
        print(f"Epoch {epoch}: w={w:.4f}, b={b:.4f}, Loss={total_loss:.4f}")

print(f"\nโ ุงููุชูุฌุฉ ุงูููุงุฆูุฉ: w={w:.4f}, b={b:.4f}")
```

---

### ๐ **ูููุฒุงุช Elastic Net**

โ ูุฌูุน ุจูู ููุฒุงุช Lasso ู Ridge.
โ ูุชุนุงูู ูุน ุงูุจูุงูุงุช ุนุงููุฉ ุงูุฃุจุนุงุฏ (High-dimensional data).
โ ูููู ูู Overfitting ููุญุณู ุงูุชุนููู (Generalization).
โ ููุงุณุจ ุนูุฏูุง ุชููู ููุงู ูููุฒุงุช ูุชุนุฏุฏุฉ ูุชุฑุงุจุทุฉ ุจุดุฏุฉ.

---

### ๐ **ุงูุฎูุงุตุฉ**

* Elastic Net ูู ูุฒูุฌ ุฐูู ุจูู **Lasso** ู **Ridge**.
* ุจุงุณุชุฎุฏุงู ูุนุงูู $\alpha$ุ ููููู ุชุฎุตูุต ููุฏุงุฑ ุงูุงุนุชูุงุฏ ุนูู L1 ุฃู L2 ุญุณุจ ูุดููุชู.
* ุนูุฏ $\alpha=1$ ูุตุจุญ Lassoุ ูุนูุฏ $\alpha=0$ ูุตุจุญ Ridge.

---

## ๐ **ุฅู ุฃุนุฌุจู ุงููุดุฑูุน ูุง ุชูุณู ุฏุนููุง ุจู Star โญ๏ธ ุนูู GitHub!**
